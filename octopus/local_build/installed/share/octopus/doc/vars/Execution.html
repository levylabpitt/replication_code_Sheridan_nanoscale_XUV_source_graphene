
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script>
<style>
   BODY {background-color: white; 
         font-size: 10pt; font-family: verdana,helvetica;}
   A  {text-decoration: none;color: blue}
</style>
</head>
<body>

<a name='Execution'</a>
<H2>Execution</H2>


<p><b><a name='FromScratch'></a>FromScratch</b>
<br/><i>Section</i>: Execution
<br/><i>Type</i>: logical
<br/><i>Default</i>: false
<br/><br> When this variable is set to true, <tt>Octopus</tt> will perform a
 calculation from the beginning, without looking for restart
 information.

</p><hr width='30%' align='left'/>

<a name='Execution::Accel'</a>
<H2>Execution::Accel</H2>


<p><b><a name='AccelBenchmark'></a>AccelBenchmark</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> If this variable is set to yes, Octopus will run some
 routines to benchmark the performance of the accelerator device.

</p><hr width='30%' align='left'/>


<p><b><a name='AccelDevice'></a>AccelDevice</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: integer
<br/><i>Default</i>: gpu
<br/><br> This variable selects the OpenCL or CUDA accelerator device
 that Octopus will use. You can specify one of the options below
 or a numerical id to select a specific device.
<br><br>
 Values >= 0 select the device to be used. In case of MPI enabled runs
 devices are distributed in a round robin fashion, starting at this value.

<br/><i>Options</i>:
<ul>
<li><b>gpu</b>:  If available, Octopus will use a GPU.
</li>
<li><b>cpu</b>:  If available, Octopus will use a CPU (only for OpenCL).
</li>
<li><b>accelerator</b>:  If available, Octopus will use an accelerator (only for OpenCL).
</li>
<li><b>accel_default</b>:  Octopus will use the default device specified by the implementation.
 implementation.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='AccelPlatform'></a>AccelPlatform</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: integer
<br/><i>Default</i>: 0
<br/><br> This variable selects the OpenCL platform that Octopus will
 use. You can give an explicit platform number or use one of
 the options that select a particular vendor
 implementation. Platform 0 is used by default.
<br><br>
 This variable has no effect for CUDA.

<br/><i>Options</i>:
<ul>
<li><b>amd</b>:  Use the AMD OpenCL platform.
</li>
<li><b>nvidia</b>:  Use the Nvidia OpenCL platform.
</li>
<li><b>ati</b>:  Use the ATI (old AMD) OpenCL platform.
</li>
<li><b>intel</b>:  Use the Intel OpenCL platform.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='AllowCPUonly'></a>AllowCPUonly</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: logical
<br/><br> In order to prevent waste of resources, the code will normally stop when the GPU is disabled due to
 incomplete implementations or incompatibilities. AllowCPUonly = yes overrides this and allows the
 code execution also in these cases.

</p><hr width='30%' align='left'/>


<p><b><a name='CudaAwareMPI'></a>CudaAwareMPI</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: logical
<br/><br> If Octopus was compiled with CUDA support and MPI support and if the MPI
 implementation is CUDA-aware (i.e., it supports communication using device pointers),
 this switch can be set to true to use the CUDA-aware MPI features. The advantage
 of this approach is that it can do, e.g., peer-to-peer copies between devices without
 going through the host memmory.
 The default is false, except when the configure switch --enable-cudampi is set, in which
 case this variable is set to true.

</p><hr width='30%' align='left'/>


<p><b><a name='DisableAccel'></a>DisableAccel</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: logical
<br/><i>Default</i>: yes
<br/><br> If Octopus was compiled with OpenCL or CUDA support, it will
 try to initialize and use an accelerator device. By setting this
 variable to <tt>yes</tt> you force Octopus not to use an accelerator even it is available.

</p><hr width='30%' align='left'/>


<p><b><a name='InitializeGPUBuffers'></a>InitializeGPUBuffers</b>
<br/><i>Section</i>: Execution::Accel
<br/><i>Type</i>: logical
<br/><br> Initialize new GPU buffers to zero on creation (use only for debugging, as it has a performance impact!).

</p><hr width='30%' align='left'/>

<a name='Execution::Debug'</a>
<H2>Execution::Debug</H2>


<p><b><a name='Debug'></a>Debug</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: flag
<br/><i>Default</i>: no
<br/><br> This variable controls the amount of debugging information
 generated by Octopus. You can use include more than one option
 with the + operator.

<br/><i>Options</i>:
<ul>
<li><b>no</b>:  (default) <tt>Octopus</tt> does not enter debug mode.
</li>
<li><b>propagation_graph</b>:  Octopus generates a file with information for the propagation diagram.
</li>
<li><b>extra_checks</b>:  This enables Octopus to perform some extra checks, to ensure
 code correctness, that might be too costly for regular runs.
</li>
<li><b>info</b>:  Octopus prints additional information to the terminal.
</li>
<li><b>trace</b>:  Octopus generates a stack trace as it enters end exits
 subroutines. This information is reported if Octopus stops with
 an error.
</li>
<li><b>interaction_graph</b>:  Octopus generates a dot file containing the graph for a multisystem run.
</li>
<li><b>trace_term</b>:  The trace is printed to the terminal as Octopus enters or exits subroutines. This slows down execution considerably.
</li>
<li><b>interaction_graph_full</b>:  Octopus generates a dot file containing the graph for a multisystem run including ghost interactions.
</li>
<li><b>trace_file</b>:  The trace is written to files in the <tt>debug</tt>
 directory. For each node (when running in parallel) there is a file called
 <tt>debug_trace.&lt;rank&gt;</tt>. Writing these files slows down the code by a huge factor and
 it is usually only necessary for parallel runs.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='DebugTrapSignals'></a>DebugTrapSignals</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: logical
<br/><i>Default</i>: yes
<br/><br> If true, trap signals to handle them in octopus itself and
 print a custom backtrace. If false, do not trap signals; then,
 core dumps can be produced or gdb can be used to stop at the
 point a signal was produced (e.g. a segmentation fault).

</p><hr width='30%' align='left'/>


<p><b><a name='ExperimentalFeatures'></a>ExperimentalFeatures</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> If true, allows the use of certain parts of the code that are
 still under development and are not suitable for production
 runs. This should not be used unless you know what you are doing.
 See details on
 <a href=http://octopus-code.org/experimental_features>wiki page</a>.

</p><hr width='30%' align='left'/>


<p><b><a name='ForceComplex'></a>ForceComplex</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> Normally <tt>Octopus</tt> determines automatically the type necessary
 for the wavefunctions. When set to yes this variable will
 force the use of complex wavefunctions.
<br><br>
 Warning: This variable is designed for testing and
 benchmarking and normal users need not use it.

</p><hr width='30%' align='left'/>


<p><b><a name='MPIDebugHook'></a>MPIDebugHook</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> When debugging the code in parallel it is usually difficult to find the origin
 of race conditions that appear in MPI communications. This variable introduces
 a facility to control separate MPI processes. If set to yes, all nodes will
 start up, but will get trapped in an endless loop. In every cycle of the loop
 each node is sleeping for one second and is then checking if a file with the
 name <tt>node_hook.xxx</tt> (where <tt>xxx</tt> denotes the node number) exists. A given node can
 only be released from the loop if the corresponding file is created. This allows
 to selectively run, <i>e.g.</i>, a compute node first followed by the master node. Or, by
 reversing the file creation of the node hooks, to run the master first followed
 by a compute node.

</p><hr width='30%' align='left'/>


<p><b><a name='ReportMemory'></a>ReportMemory</b>
<br/><i>Section</i>: Execution::Debug
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> If true, after each SCF iteration <tt>Octopus</tt> will print
 information about the memory the code is using. The quantity
 reported is an approximation to the size of the heap and
 generally it is a lower bound to the actual memory <tt>Octopus</tt> is
 using.

</p><hr width='30%' align='left'/>

<a name='Execution::IO'</a>
<H2>Execution::IO</H2>


<p><b><a name='MaxwellRestartWriteInterval'></a>MaxwellRestartWriteInterval</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: integer
<br/><i>Default</i>: 50
<br/><br> Restart data is written when the iteration number is a multiple of the
 <tt>MaxwellRestartWriteInterval</tt> variable. (Other output is controlled by <tt>MaxwellOutputInterval</tt>.)

</p><hr width='30%' align='left'/>


<p><b><a name='RestartOptions'></a>RestartOptions</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: block
<br/><br> <tt>Octopus</tt> usually stores binary information, such as the wavefunctions, to be used
 in subsequent calculations. The most common example is the ground-state states
 that are used to start a time-dependent calculation. This variable allows to control
 where this information is written to or read from. The format of this block is the following:
 for each line, the first column indicates the type of data, the second column indicates
 the path to the directory that should be used to read and write that restart information, and the
 third column, which is optional, allows one to set some flags to modify the way how the data
 is read or written. For example, if you are running a time-dependent calculation, you can
 indicate where <tt>Octopus</tt> can find the ground-state information in the following way:
<br><br>
 <tt>%RestartOptions
 <br>&nbsp;&nbsp;restart_gs | "gs_restart"
 <br>&nbsp;&nbsp;restart_td | "td_restart"
 <br>%</tt>
<br><br>
 The second line of the above example also tells <tt>Octopus</tt> that the time-dependent restart data
 should be read from and written to the "td_restart" directory.
<br><br>
 In case you want to change the path of all the restart directories, you can use the <tt>restart_all</tt> option.
 When using the <tt>restart_all</tt> option, it is still possible to have a different restart directory for specific
 data types. For example, when including the following block in your input file:
<br><br>
 <tt>%RestartOptions
 <br>&nbsp;&nbsp;restart_all | "my_restart"
 <br>&nbsp;&nbsp;restart_td&nbsp;  | "td_restart"
 <br>%</tt>
<br><br>
 the time-dependent restart information will be stored in the "td_restart" directory, while all the remaining
 restart information will be stored in the "my_restart" directory.
<br><br>
 By default, the name of the "restart_all" directory is set to "restart".
<br><br>
 Some <tt>CalculationMode</tt>s also take into account specific flags set in the third column of the <tt>RestartOptions</tt>
 block. These are used to determine if some specific part of the restart data is to be taken into account
 or not when reading the restart information. For example, when restarting a ground-state calculation, one can
 set the <tt>restart_rho</tt> flags, so that the density used is not built from the saved wavefunctions, but is
 instead read from the restart directory. In this case, the block should look like this:
<br><br>
 <tt>%RestartOptions
 <br>&nbsp;&nbsp;restart_gs | "restart" | restart_rho
 <br>%</tt>
<br><br>
 A list of available flags is given below, but note that the code might ignore some of them, which will happen if they
 are not available for that particular calculation, or might assume some of them always present, which will happen
 in case they are mandatory.
<br><br>
 Finally, note that all the restart information of a given data type is always stored in a subdirectory of the
 specified path. The name of this subdirectory is fixed and cannot be changed. For example, ground-state information
 will always be stored in a subdirectory named "gs". This makes it safe in most situations to use the same path for
 all the data types. The name of these subdirectories is indicated in the description of the data types below.
<br><br>
 Currently, the available restart data types and flags are the following:

<br/><i>Options</i>:
<ul>
<li><b>restart_all</b>:  (data type)
 Option to globally change the path of all the restart information.
</li>
<li><b>restart_oct</b>:  (data type)
 The data for optimal control calculations.
 This information is stored under the "opt-control" subdirectory.
</li>
<li><b>restart_partition</b>:  (data type)
 The data for the mesh partitioning.
 This information is stored under the "partition" subdirectory.
</li>
<li><b>restart_proj</b>:  (data type)
 The ground-state to be used with the td_occup and populations options of <tt>TDOutput</tt>.
 This information should be a ground state, so the "gs" subdirectory is used.
</li>
<li><b>restart_skip</b>:  (flag)
 This flag allows to selectively skip the reading and writting of specific restart information.
</li>
<li><b>restart_gs</b>:  (data type)
 The data resulting from a ground-state calculation.
 This information is stored under the "gs" subdirectory.
</li>
<li><b>restart_states</b>:  (flag)
 Read the electronic states. (not yet implemented)
</li>
<li><b>restart_rho</b>:  (flag)
 Read the electronic density.
</li>
<li><b>restart_unocc</b>:  (data type)
 The data resulting from an unoccupied states calculation. This information also corresponds to a ground state and
 can be used as such, so it is stored under the same subdirectory as the one of restart_gs.
</li>
<li><b>restart_td</b>:  (data type)
 The data resulting from a real-time time-dependent calculation.
 This information is stored under the "td" subdirectory.
</li>
<li><b>restart_em_resp</b>:  (data type)
 The data resulting from the calculation of the electromagnetic response using the Sternheimer approach.
 This information is stored under the "em_resp" subdirectory.
</li>
<li><b>restart_vhxc</b>:  (flag)
 Read the Hartree and XC potentials.
</li>
<li><b>restart_em_resp_fd</b>:  (data type)
 The data resulting from the calculation of the electromagnetic response using finite-differences.
 This information is stored under the "em_resp_fd" subdirectory.
</li>
<li><b>restart_kdotp</b>:  (data type)
 The data resulting from the calculation of effective masses by k.p perturbation theory.
 This information is stored under the "kdotp" subdirectory.
</li>
<li><b>restart_vib_modes</b>:  (data type)
 The data resulting from the calculation of vibrational modes.
 This information is stored under the "vib_modes" subdirectory.
</li>
<li><b>restart_mix</b>:  (flag)
 Read the SCF mixing information.
</li>
<li><b>restart_vdw</b>:  (data type)
 The data resulting from the calculation of van der Waals coefficients.
 This information is stored under the "vdw" subdirectory.
</li>
<li><b>restart_casida</b>:  (data type)
 The data resulting from a Casida calculation.
 This information is stored under the "casida" subdirectory.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='RestartWallTimePeriod'></a>RestartWallTimePeriod</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: float
<br/><i>Default</i>: 120
<br/><br> Period Time (in minutes) at which the restart file will be written.
 If a finite time (in minutes) is specified, the code will write the restart file every period.

</p><hr width='30%' align='left'/>


<p><b><a name='RestartWrite'></a>RestartWrite</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: logical
<br/><i>Default</i>: true
<br/><br> If this variable is set to no, restart information is not
 written. Note that some run modes will ignore this
 option and write some restart information anyway.

</p><hr width='30%' align='left'/>


<p><b><a name='RestartWriteInterval'></a>RestartWriteInterval</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: integer
<br/><i>Default</i>: 50
<br/><br> Restart data is written when the iteration number is a multiple
 of the <tt>RestartWriteInterval</tt> variable. For
 time-dependent runs this includes the update of the output
 controlled by the <tt>TDOutput</tt> variable. (Other output is
 controlled by <tt>OutputInterval</tt>.)

</p><hr width='30%' align='left'/>


<p><b><a name='RestartWriteTime'></a>RestartWriteTime</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: float
<br/><i>Default</i>: 5
<br/><br> The RestartWriteTime (in minutes) will be subtracted from the WallTime to allow time for writing the restart file.
 In huge calculations, this value should be increased.

</p><hr width='30%' align='left'/>


<p><b><a name='SlakoDir'></a>SlakoDir</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: string
<br/><i>Default</i>: "./"
<br/><br> Folder containing the Slako files

</p><hr width='30%' align='left'/>


<p><b><a name='Walltime'></a>Walltime</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: float
<br/><i>Default</i>: 0
<br/><br> Time in minutes before which the restart file will be written. This is to make sure that at least one restart
 file can be written before the code might be killed to to exceeding the given CPU time.
 If a finite time (in minutes) is specified, the code will write the restart file when the next
 iteration (plus the RestartWriteTime) would exceed the given time.
 A value less than 1 second (1/60 minutes) will disable the timer.

</p><hr width='30%' align='left'/>


<p><b><a name='WorkDir'></a>WorkDir</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: string
<br/><i>Default</i>: "."
<br/><br> By default, all files are written and read from the working directory,
 <i>i.e.</i> the directory from which the executable was launched. This behavior can
 be changed by setting this variable. If you set <tt>WorkDir</tt> to a name other than ".",
 the following directories are written and read in that directory:
<ul>
 <li>"casida/"</li>
 <li>"em_resp_fd/"</li>
 <li>"em_resp/"</li>
 <li>"geom/"</li>
 <li>"kdotp/"</li>
 <li>"local.general"</li>
 <li>"pcm/"</li>
 <li>"profiling/"</li>
 <li>"restart/"</li>
 <li>"static/"</li>
 <li>"td.general/"</li>
 <li>"vdw/"</li>
 <li>"vib_modes/"</li>
</ul>
 Furthermore, some of the debug information (see <tt>Debug</tt>) is also written to <tt>WorkDir</tt> and
 the non-absolute paths defined in <tt>OutputIterDir</tt> are relative to <tt>WorkDir</tt>.

</p><hr width='30%' align='left'/>


<p><b><a name='stderr'></a>stderr</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: string
<br/><i>Default</i>: "-"
<br/><br> The standard error by default goes to, well, to standard error. This can
 be changed by setting this variable: if you give it a name (other than "-")
 the output stream is printed in that file instead.

</p><hr width='30%' align='left'/>


<p><b><a name='stdout'></a>stdout</b>
<br/><i>Section</i>: Execution::IO
<br/><i>Type</i>: string
<br/><i>Default</i>: "-"
<br/><br> The standard output by default goes to, well, to standard output. This can
 be changed by setting this variable: if you give it a name (other than "-")
 the output stream is printed in that file instead.

</p><hr width='30%' align='left'/>

<a name='Execution::Optimization'</a>
<H2>Execution::Optimization</H2>


<p><b><a name='HamiltonianApplyPacked'></a>HamiltonianApplyPacked</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><i>Default</i>: yes
<br/><br> If set to yes (the default), Octopus will 'pack' the
 wave-functions when operating with them. This might involve some
 additional copying but makes operations more efficient.
 See also the related <tt>StatesPack</tt> variable.

</p><hr width='30%' align='left'/>


<p><b><a name='MemoryLimit'></a>MemoryLimit</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: -1
<br/><br> If positive, <tt>Octopus</tt> will stop if more memory than <tt>MemoryLimit</tt>
 is requested (in kb). Note that this variable only works when
 <tt>ProfilingMode = prof_memory(_full)</tt>.

</p><hr width='30%' align='left'/>


<p><b><a name='MeshBlockDirection'></a>MeshBlockDirection</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><br> Determines the direction in which the dimensions are chosen to compute
 the blocked index for sorting the mesh points (see MeshBlockSize).
 The default is increase_with_dimensions, corresponding to xyz ordering
 in 3D.

<br/><i>Options</i>:
<ul>
<li><b>increase_with_dimension</b>:  The fastest changing index is in the first dimension, i.e., in 3D this
 corresponds to ordering in xyz directions.
</li>
<li><b>decrease_with_dimension</b>:  The fastest changing index is in the last dimension, i.e., in 3D this
 corresponds to ordering in zyx directions.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshBlockSize'></a>MeshBlockSize</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: block
<br/><br> To improve memory-access locality when calculating derivatives,
 <tt>Octopus</tt> arranges mesh points in blocks. This variable
 controls the size of this blocks in the different
 directions. The default is selected according to the value of
 the <tt>StatesBlockSize</tt> variable. (This variable only affects the
 performance of <tt>Octopus</tt> and not the results.)

</p><hr width='30%' align='left'/>


<p><b><a name='MeshLocalBlockDirection'></a>MeshLocalBlockDirection</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><br> Determines the direction in which the dimensions are chosen to compute
 the blocked index for sorting the mesh points (see MeshLocalBlockSize).
 The default is increase_with_dimensions, corresponding to xyz ordering
 in 3D.

<br/><i>Options</i>:
<ul>
<li><b>increase_with_dimension</b>:  The fastest changing index is in the first dimension, i.e., in 3D this
 corresponds to ordering in xyz directions.
</li>
<li><b>decrease_with_dimension</b>:  The fastest changing index is in the last dimension, i.e., in 3D this
 corresponds to ordering in zyx directions.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshLocalBlockSize'></a>MeshLocalBlockSize</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: block
<br/><br> To improve memory-access locality when calculating derivatives,
 <tt>Octopus</tt> arranges mesh points in blocks. This variable
 controls the size of this blocks in the different
 directions. The default is selected according to the value of
 the <tt>StatesBlockSize</tt> variable. (This variable only affects the
 performance of <tt>Octopus</tt> and not the results.)

</p><hr width='30%' align='left'/>


<p><b><a name='MeshLocalOrder'></a>MeshLocalOrder</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: blocks
<br/><br> This variable controls how the grid points are mapped to a
 linear array. This influences the performance of the code.

<br/><i>Options</i>:
<ul>
<li><b>order_blocks</b>:  The grid is mapped using small parallelepipedic grids. The size
 of the blocks is controlled by <tt>MeshBlockSize</tt>.
</li>
<li><b>order_cube</b>:  The grid is mapped using a full cube, i.e. without blocking.
</li>
<li><b>order_global</b>:  Use the ordering from the global mesh
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshOrder'></a>MeshOrder</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><br> This variable controls how the grid points are mapped to a
 linear array for global arrays. For runs that are parallel
 in domains, the local mesh order may be different (see
 <tt>MeshLocalOrder</tt>).
 The default is blocks when serial in domains and cube when
 parallel in domains with the local mesh order set to blocks.

<br/><i>Options</i>:
<ul>
<li><b>order_blocks</b>:  The grid is mapped using small parallelepipedic grids. The size
 of the blocks is controlled by <tt>MeshBlockSize</tt>.
</li>
<li><b>order_original</b>:  The original order of the indices is used to map the grid.
</li>
<li><b>order_cube</b>:  The grid is mapped using a full cube, i.e. without blocking.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='NLOperatorCompactBoundaries'></a>NLOperatorCompactBoundaries</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> (Experimental) When set to yes, for finite systems Octopus will
 map boundary points for finite-differences operators to a few
 memory locations. This increases performance, however it is
 experimental and has not been thoroughly tested.

</p><hr width='30%' align='left'/>


<p><b><a name='OperateAccel'></a>OperateAccel</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: map
<br/><br> This variable selects the subroutine used to apply non-local
 operators over the grid when an accelerator device is used.

<br/><i>Options</i>:
<ul>
<li><b>invmap</b>:  The standard implementation ported to OpenCL.
</li>
<li><b>map</b>:  A different version, more suitable for GPUs.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='OperateComplex'></a>OperateComplex</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: optimized
<br/><br> This variable selects the subroutine used to apply non-local
 operators over the grid for complex functions.

<br/><i>Options</i>:
<ul>
<li><b>fortran</b>:  The standard Fortran function.
</li>
<li><b>optimized</b>:  This version is optimized using vector primitives (if available).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='OperateComplexSingle'></a>OperateComplexSingle</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: optimized
<br/><br> This variable selects the subroutine used to apply non-local
 operators over the grid for single-precision complex functions.

<br/><i>Options</i>:
<ul>
<li><b>fortran</b>:  The standard Fortran function.
</li>
<li><b>optimized</b>:  This version is optimized using vector primitives (if available).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='OperateDouble'></a>OperateDouble</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: optimized
<br/><br> This variable selects the subroutine used to apply non-local
 operators over the grid for real functions.

<br/><i>Options</i>:
<ul>
<li><b>fortran</b>:  The standard Fortran function.
</li>
<li><b>optimized</b>:  This version is optimized using vector primitives (if available).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='OperateSingle'></a>OperateSingle</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: optimized
<br/><br> This variable selects the subroutine used to apply non-local
 operators over the grid for single-precision real functions.

<br/><i>Options</i>:
<ul>
<li><b>fortran</b>:  The standard Fortran function.
</li>
<li><b>optimized</b>:  This version is optimized using vector primitives (if available).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ProfilingAllNodes'></a>ProfilingAllNodes</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> This variable controls whether all nodes print the time
 profiling output. If set to no, the default, only the root node
 will write the profile. If set to yes, all nodes will print it.

</p><hr width='30%' align='left'/>


<p><b><a name='ProfilingMode'></a>ProfilingMode</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><i>Default</i>: no
<br/><br> Use this variable to run <tt>Octopus</tt> in profiling mode. In this mode
 <tt>Octopus</tt> records the time spent in certain areas of the code and
 the number of times this code is executed. These numbers
 are written in <tt>./profiling.NNN/profiling.nnn</tt> with <tt>nnn</tt> being the
 node number (<tt>000</tt> in serial) and <tt>NNN</tt> the number of processors.
 This is mainly for development purposes. Note, however, that
 <tt>Octopus</tt> should be compiled with <tt>--disable-debug</tt> to do proper
 profiling. Warning: you may encounter strange results with OpenMP.

<br/><i>Options</i>:
<ul>
<li><b>no</b>:  No profiling information is generated.
</li>
<li><b>prof_io</b>:  Count the number of file open and close.
</li>
<li><b>prof_time</b>:  Profile the time spent in defined profiling regions.
</li>
<li><b>prof_memory</b>:  As well as the time, summary information on memory usage and the largest arrays are reported.
</li>
<li><b>prof_memory_full</b>:  As well as the time and summary memory information, a
 log is reported of every allocation and deallocation.
</li>
<li><b>likwid</b>:  Enable instrumentation using LIKWID.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ProfilingOutputTree'></a>ProfilingOutputTree</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><i>Default</i>: yes
<br/><br> This variable controls whether the profiling output is additionally
 written as a tree.

</p><hr width='30%' align='left'/>


<p><b><a name='ProfilingOutputYAML'></a>ProfilingOutputYAML</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> This variable controls whether the profiling output is additionally
 written to a YAML file.

</p><hr width='30%' align='left'/>


<p><b><a name='StatesBlockSize'></a>StatesBlockSize</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: integer
<br/><br> Some routines work over blocks of eigenfunctions, which
 generally improves performance at the expense of increased
 memory consumption. This variable selects the size of the
 blocks to be used. If GPUs are used, the default is 32;
 otherwise it is 4.

</p><hr width='30%' align='left'/>


<p><b><a name='StatesCLDeviceMemory'></a>StatesCLDeviceMemory</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: float
<br/><i>Default</i>: -512
<br/><br> This variable selects the amount of OpenCL device memory that
 will be used by Octopus to store the states.
<br><br>
 A positive number smaller than 1 indicates a fraction of the total
 device memory. A number larger than one indicates an absolute
 amount of memory in megabytes. A negative number indicates an
 amount of memory in megabytes that would be subtracted from
 the total device memory.

</p><hr width='30%' align='left'/>


<p><b><a name='StatesPack'></a>StatesPack</b>
<br/><i>Section</i>: Execution::Optimization
<br/><i>Type</i>: logical
<br/><br> When set to yes, states are stored in packed mode, which improves
 performance considerably. Not all parts of the code will profit from
 this, but should nevertheless work regardless of how the states are
 stored.
<br><br>
 If GPUs are used and this variable is set to yes, Octopus
 will store the wave-functions in device (GPU) memory. If
 there is not enough memory to store all the wave-functions,
 execution will stop with an error.
<br><br>
 See also the related <tt>HamiltonianApplyPacked</tt> variable.
<br><br>
 The default is yes.

</p><hr width='30%' align='left'/>

<a name='Execution::Parallelization'</a>
<H2>Execution::Parallelization</H2>


<p><b><a name='MeshPartition'></a>MeshPartition</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><br> When using METIS to perform the mesh partitioning, decides which
 algorithm is used. By default, <tt>graph</tt> partitioning
 is used for 8 or more partitions, and <tt>rcb</tt> for fewer.

<br/><i>Options</i>:
<ul>
<li><b>rcb</b>:  Recursive coordinate bisection partitioning.
</li>
<li><b>graph</b>:  Graph partitioning (called 'k-way' by METIS).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshPartitionPackage'></a>MeshPartitionPackage</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><br> Decides which library to use to perform the mesh partition.
 By default ParMETIS is used when available, otherwise METIS is used.

<br/><i>Options</i>:
<ul>
<li><b>metis</b>:  METIS library.
</li>
<li><b>parmetis</b>:  (Experimental) Use ParMETIS libary to perform the mesh partition.
 Only available if the code was compiled with ParMETIS support.
</li>
<li><b>part_hilbert</b>:  Use the ordering along the Hilbert curve for partitioning.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshPartitionStencil'></a>MeshPartitionStencil</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: stencil_star
<br/><br> To partition the mesh, it is necessary to calculate the connection
 graph connecting the points. This variable selects which stencil
 is used to do this.

<br/><i>Options</i>:
<ul>
<li><b>stencil_star</b>:  An order-one star stencil.
</li>
<li><b>laplacian</b>:  The stencil used for the Laplacian is used to calculate the
 partition. This in principle should give a better partition, but
 it is slower and requires more memory.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='MeshPartitionVirtualSize'></a>MeshPartitionVirtualSize</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: mesh mpi_grp size
<br/><br> Gives the possibility to change the partition nodes.
 Afterward, it crashes.

</p><hr width='30%' align='left'/>


<p><b><a name='MeshUseTopology'></a>MeshUseTopology</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><i>Default</i>: false
<br/><br> (experimental) If enabled, <tt>Octopus</tt> will use an MPI virtual
 topology to map the processors. This can improve performance
 for certain interconnection systems.

</p><hr width='30%' align='left'/>


<p><b><a name='ParDomains'></a>ParDomains</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: auto
<br/><br> This variable controls the number of processors used for the
 parallelization in domains.
 The special value <tt>auto</tt>, the default, lets Octopus
 decide how many processors will be assigned for this
 strategy. To disable parallelization in domains, you can use
 <tt>ParDomains = no</tt> (or set the number of processors to
 1).
<br><br>
 The total number of processors required is the multiplication
 of the processors assigned to each parallelization strategy.

<br/><i>Options</i>:
<ul>
<li><b>auto</b>:  The number of processors is assigned automatically.
</li>
<li><b>no</b>:  This parallelization strategy is not used.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ParKPoints'></a>ParKPoints</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: auto
<br/><br> This variable controls the number of processors used for the
 parallelization in K-Points and/or spin.
 The special value <tt>auto</tt> lets Octopus decide how many processors will be
 assigned for this strategy. To disable parallelization in
 KPoints, you can use <tt>ParKPoints = no</tt> (or set the
 number of processors to 1).
<br><br>
 The total number of processors required is the multiplication
 of the processors assigned to each parallelization strategy.

<br/><i>Options</i>:
<ul>
<li><b>auto</b>:  The number of processors is assigned automatically.
</li>
<li><b>no</b>:  This parallelization strategy is not used.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ParOther'></a>ParOther</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: auto
<br/><br> This variable controls the number of processors used for the
 'other' parallelization mode, that is CalculatioMode
 dependent. For <tt>CalculationMode = casida</tt>, it means
 parallelization in electron-hole pairs.
<br><br>
 The special value <tt>auto</tt>,
 the default, lets Octopus decide how many processors will be
 assigned for this strategy. To disable parallelization in
 Other, you can use <tt>ParOther = no</tt> (or set the
 number of processors to 1).
<br><br>
 The total number of processors required is the multiplication
 of the processors assigned to each parallelization strategy.

<br/><i>Options</i>:
<ul>
<li><b>auto</b>:  The number of processors is assigned automatically.
</li>
<li><b>no</b>:  This parallelization strategy is not used.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ParStates'></a>ParStates</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><br> This variable controls the number of processors used for the
 parallelization in states. The special value <tt>auto</tt> lets
 Octopus decide how many processors will be assigned for this
 strategy. To disable parallelization in states, you can use
 <tt>ParStates = no</tt> (or set the number of processors to 1).
<br><br>
 The default value depends on the <tt>CalculationMode</tt>. For
 <tt>CalculationMode = td</tt> the default is <tt>auto</tt>, while
 for for other modes the default is <tt>no</tt>.
<br><br>
 The total number of processors required is the multiplication
 of the processors assigned to each parallelization strategy.

<br/><i>Options</i>:
<ul>
<li><b>auto</b>:  The number of processors is assigned automatically.
</li>
<li><b>no</b>:  This parallelization strategy is not used.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ParallelXC'></a>ParallelXC</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><i>Default</i>: true
<br/><br> When enabled, additional parallelization
 will be used for the calculation of the XC functional.

</p><hr width='30%' align='left'/>


<p><b><a name='ParallelizationNumberSlaves'></a>ParallelizationNumberSlaves</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: 0
<br/><br> Slaves are nodes used for task parallelization. The number of
 such nodes is given by this variable multiplied by the number
 of domains used in domain parallelization.

</p><hr width='30%' align='left'/>


<p><b><a name='ParallelizationOfDerivatives'></a>ParallelizationOfDerivatives</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: integer
<br/><i>Default</i>: non_blocking
<br/><br> This option selects how the communication of mesh boundaries is performed.

<br/><i>Options</i>:
<ul>
<li><b>blocking</b>:  Blocking communication.
</li>
<li><b>non_blocking</b>:  Communication is based on non-blocking point-to-point communication.
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='ParallelizationPoissonAllNodes'></a>ParallelizationPoissonAllNodes</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><i>Default</i>: true
<br/><br> When running in parallel, this variable selects whether the
 Poisson solver should divide the work among all nodes or only
 among the parallelization-in-domains groups.

</p><hr width='30%' align='left'/>


<p><b><a name='PartitionPrint'></a>PartitionPrint</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><i>Default</i>: true
<br/><br> (experimental) If disabled, <tt>Octopus</tt> will not compute
 nor print the partition information, such as local points,
 no. of neighbours, ghost points and boundary points.

</p><hr width='30%' align='left'/>


<p><b><a name='ReorderRanks'></a>ReorderRanks</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><i>Default</i>: no
<br/><br> This variable controls whether the ranks are reorganized to have a more
 compact distribution with respect to domain parallelization which needs
 to communicate most often. Depending on the system, this can improve
 communication speeds.

</p><hr width='30%' align='left'/>


<p><b><a name='ScaLAPACKCompatible'></a>ScaLAPACKCompatible</b>
<br/><i>Section</i>: Execution::Parallelization
<br/><i>Type</i>: logical
<br/><br> Whether to use a layout for states parallelization which is compatible with ScaLAPACK.
 The default is yes for <tt>CalculationMode = gs, unocc, go</tt> without k-point parallelization,
 and no otherwise. (Setting to other than default is experimental.)
 The value must be yes if any ScaLAPACK routines are called in the course of the run;
 it must be set by hand for <tt>td</tt> with <tt>TDDynamics = bo</tt>.
 This variable has no effect unless you are using states parallelization and have linked ScaLAPACK.
 Note: currently, use of ScaLAPACK is not compatible with task parallelization (<i>i.e.</i> slaves).

</p><hr width='30%' align='left'/>

<a name='Execution::Symmetries'</a>
<H2>Execution::Symmetries</H2>


<p><b><a name='SymmetriesCompute'></a>SymmetriesCompute</b>
<br/><i>Section</i>: Execution::Symmetries
<br/><i>Type</i>: logical
<br/><br> If disabled, <tt>Octopus</tt> will not compute
 nor print the symmetries.
<br><br>
 By default, symmetries are computed when running in 3
 dimensions for systems with less than 100 atoms.
 For periodic systems, the default is always true, irrespective of the number of atoms.

</p><hr width='30%' align='left'/>


<p><b><a name='SymmetriesTolerance'></a>SymmetriesTolerance</b>
<br/><i>Section</i>: Execution::Symmetries
<br/><i>Type</i>: float
<br/><br> For periodic systems, this variable controls the tolerance used by the symmetry finder
 (spglib) to find the spacegroup and symmetries of the crystal.

</p><hr width='30%' align='left'/>

<a name='Execution::Units'</a>
<H2>Execution::Units</H2>


<p><b><a name='Units'></a>Units</b>
<br/><i>Section</i>: Execution::Units
<br/><i>Type</i>: virtual
<br/><i>Default</i>: atomic
<br/><br> (Virtual) These are the units that can be used in the input file.
<br><br>

</p><hr width='30%' align='left'/>


<p><b><a name='UnitsOutput'></a>UnitsOutput</b>
<br/><i>Section</i>: Execution::Units
<br/><i>Type</i>: integer
<br/><i>Default</i>: atomic
<br/><br> This variable selects the units that Octopus use for output.
<br><br>
 Atomic units seem to be the preferred system in the atomic and
 molecular physics community. Internally, the code works in
 atomic units. However, for output, some people like
 to use a system based on electron-Volts (eV) for energies
 and Angstroms (&Aring;) for length.
<br><br>
 Normally time units are derived from energy and length units,
 so it is measured in \(\hbar\)/Hartree or
 \(\hbar\)/eV.
<br><br>
 Warning 1: All files read on input will also be treated using
 these units, including XYZ geometry files.
<br><br>
 Warning 2: Some values are treated in their most common units,
 for example atomic masses (a.m.u.), electron effective masses
 (electron mass), vibrational frequencies
 (cm<sup>-1</sup>) or temperatures (Kelvin). The unit of charge is always
 the electronic charge <i>e</i>.
<br><br>

<br/><i>Options</i>:
<ul>
<li><b>atomic</b>:  Atomic units.
</li>
<li><b>ev_angstrom</b>:  Electronvolts for energy, Angstroms for length, the rest of the
 units are derived from these and \(\hbar=1\).
</li>
</ul>
</p><hr width='30%' align='left'/>


<p><b><a name='UnitsXYZFiles'></a>UnitsXYZFiles</b>
<br/><i>Section</i>: Execution::Units
<br/><i>Type</i>: integer
<br/><i>Default</i>: angstrom_units
<br/><br> This variable selects in which units I/O of XYZ files should be
 performed.

<br/><i>Options</i>:
<ul>
<li><b>bohr_units</b>:  The XYZ will be assumed to be in Bohr atomic units.
</li>
<li><b>angstrom_units</b>:  XYZ files will be assumed to be always in Angstrom,
 independently of the units used by Octopus. This ensures
 compatibility with most programs, that assume XYZ files have
 coordinates in Angstrom.
</li>
</ul>
</p><hr width='30%' align='left'/>

</body>
</html>